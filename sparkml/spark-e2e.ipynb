{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "import ads\n",
    "import os \n",
    "from ads.common.model_export_util import prepare_generic_model\n",
    "\n",
    "# create a spark session: \n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('Titanic') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read a csv from object storage\n",
    "\n",
    "convert to spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option('header', 'true').load(\"oci://hosted-ds-datasets@bigdatadatasciencelarge/titanic/titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column types: \n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Typecast columns. Survived is the target variable: \n",
    "dataset = df.select(col('Survived').cast('float'),\n",
    "                         col('Pclass').cast('float'),\n",
    "                         col('Sex'),\n",
    "                         col('Age').cast('float'),\n",
    "                         col('Fare').cast('float')\n",
    "                        )\n",
    "\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index categorical columns with StringIndexer\n",
    "\n",
    "dataset = StringIndexer(inputCol='Sex', \n",
    "                        outputCol='Gender', \n",
    "                        handleInvalid='keep').fit(dataset).transform(dataset)\n",
    "\n",
    "# drop columns \n",
    "dataset = dataset.drop('Sex')\n",
    "dataset = dataset.drop('Embarked')\n",
    "\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a simple Pipeline Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input features: \n",
    "required_features = ['Pclass',\n",
    "                    'Age',\n",
    "                    'Fare',\n",
    "                    'Gender']\n",
    "\n",
    "# 80/20 data split \n",
    "(training_data, test_data) = dataset.randomSplit([0.8,0.2])\n",
    "\n",
    "assembler = VectorAssembler(inputCols=required_features, outputCol='features')\n",
    "rf = RandomForestClassifier(labelCol='Survived', \n",
    "                            featuresCol='features',\n",
    "                            maxDepth=5)\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = pipeline.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pipeline_model.transform(dataset)\n",
    "results.limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Model Object to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_dir = \"./model-artifact\"\n",
    "\n",
    "if not os.path.exists(artifact_dir):\n",
    "    os.makedirs(artifact_dir)\n",
    "pipeline_model.write().overwrite().save(f\"{artifact_dir}/my-model\")\n",
    "#pipeline_model.write().overwrite().save(\"oci://<my-bucket>@<my-namespace/my-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing a model artifact with ADS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads_artifact = prepare_generic_model(artifact_dir, \n",
    "                                     force_overwrite=True, \n",
    "                                     function_artifacts=False, \n",
    "                                     data_science_env=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overwriting score.py to support spark models \n",
    "\n",
    "Added a bunch of print statements for logging purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {artifact_dir}/score.py\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "sc = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('Titanic2') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\"\"\"\n",
    "   Inference script. This script is used for prediction by scoring server when schema is known.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"\n",
    "    Loads model from the serialized format\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model:  a model instance on which predict API can be invoked\n",
    "    \"\"\"\n",
    "    print('loading model')\n",
    "    # use this path for model deployment: \n",
    "    pm2 = PipelineModel.load('/home/datascience/model-server/app/deployed_model/my-model')\n",
    "    # use this path for testing locally: \n",
    "    #pm2 = PipelineModel.load('/home/datascience/sparkml/model-artifact/my-model/')\n",
    " \n",
    "    print('done reading model')\n",
    "\n",
    "    print(pm2.__class__)\n",
    "    return pm2\n",
    "\n",
    "\n",
    "def predict(data, model=load_model()) -> dict:\n",
    "    \"\"\"\n",
    "    Returns prediction given the model and data to predict\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: Model instance returned by load_model API\n",
    "    data: Data format as expected by the predict API of the core estimator. For eg. in case of sckit models it could be numpy array/List of list/Panda DataFrame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predictions: Output from scoring server\n",
    "        Format: { 'prediction': output from `model.predict` method }\n",
    "\n",
    "    \"\"\"\n",
    "    print('before reading data')\n",
    "    tmp = sc.read.json(sc.sparkContext.parallelize([data]))\n",
    "    print(tmp.show())\n",
    "    print(tmp.printSchema())\n",
    "    print('after reading data')\n",
    "    tmp = tmp.select(col('Pclass').cast('float'),\n",
    "                      col('Gender').cast('double'),\n",
    "                      col('Age').cast('float'),\n",
    "                      col('Fare').cast('float')\n",
    "                     )\n",
    "    print('after selecting subset of columns and typecasting')   \n",
    "    results = model.transform(tmp)\n",
    "    print('after prediction') \n",
    "    print(results.show())\n",
    "    res = results.toPandas()['prediction']\n",
    "    print(\"prediction = \", res)\n",
    "    return { 'prediction': str(list(res)) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model locally "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.insert(0, artifact_dir)\n",
    "\n",
    "from score import load_model, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample payload as a json string\n",
    "test = test_data.toPandas()\n",
    "testjson = test[:1].to_json(orient='records',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = predict(testjson, _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the artifact to the Catalog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads_artifact.save(display_name=\"my-spark-model\", \n",
    "                  description=\"pipeline object\",\n",
    "                  project_id=os.environ['PROJECT_OCID'],\n",
    "                  compartment_id=os.environ['NB_SESSION_COMPARTMENT_OCID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the model through the console "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoke the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import oci\n",
    "from oci.signer import Signer\n",
    "\n",
    "# User principal auth: \n",
    "#config = oci.config.from_file(\"~/.oci/config\") # replace with the location of your oci config file\n",
    "#auth = Signer(\n",
    "#  tenancy=config['tenancy'],\n",
    "#  user=config['user'],\n",
    "#  fingerprint=config['fingerprint'],\n",
    "#  private_key_file_location=config['key_file'],\n",
    "#  pass_phrase=config['pass_phrase'])\n",
    "\n",
    "# Resource principal auth:\n",
    "auth = oci.auth.signers.get_resource_principals_signer()\n",
    "\n",
    "# replace with your own endpoint: \n",
    "endpoint = ''\n",
    "\n",
    "requests.post(endpoint, json=testjson, auth=auth).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "requests.post(endpoint, json=testjson, auth=auth).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspv10]",
   "language": "python",
   "name": "conda-env-pyspv10-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
