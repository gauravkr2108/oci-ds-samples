{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "import ads\n",
    "import os \n",
    "from ads.common.model_export_util import prepare_generic_model\n",
    "\n",
    "# create a spark session: \n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('Titanic') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read a csv from object storage\n",
    "\n",
    "convert to spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option('header', 'true').load(\"oci://hosted-ds-datasets@bigdatadatasciencelarge/titanic/titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------------------+------+---+-----------------------+-----------------------+-------+\n",
      "|Survived|Pclass|                Name|   Sex|Age|Siblings/Spouses Aboard|Parents/Children Aboard|   Fare|\n",
      "+--------+------+--------------------+------+---+-----------------------+-----------------------+-------+\n",
      "|       0|     3|Mr. Owen Harris B...|  male| 22|                      1|                      0|   7.25|\n",
      "|       1|     1|Mrs. John Bradley...|female| 38|                      1|                      0|71.2833|\n",
      "|       1|     3|Miss. Laina Heikk...|female| 26|                      0|                      0|  7.925|\n",
      "|       1|     1|Mrs. Jacques Heat...|female| 35|                      1|                      0|   53.1|\n",
      "|       0|     3|Mr. William Henry...|  male| 35|                      0|                      0|   8.05|\n",
      "+--------+------+--------------------+------+---+-----------------------+-----------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Survived', 'string'),\n",
       " ('Pclass', 'string'),\n",
       " ('Name', 'string'),\n",
       " ('Sex', 'string'),\n",
       " ('Age', 'string'),\n",
       " ('Siblings/Spouses Aboard', 'string'),\n",
       " ('Parents/Children Aboard', 'string'),\n",
       " ('Fare', 'string')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column types: \n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----+-------+\n",
      "|Survived|Pclass|   Sex| Age|   Fare|\n",
      "+--------+------+------+----+-------+\n",
      "|     0.0|   3.0|  male|22.0|   7.25|\n",
      "|     1.0|   1.0|female|38.0|71.2833|\n",
      "|     1.0|   3.0|female|26.0|  7.925|\n",
      "|     1.0|   1.0|female|35.0|   53.1|\n",
      "|     0.0|   3.0|  male|35.0|   8.05|\n",
      "|     0.0|   3.0|  male|27.0| 8.4583|\n",
      "|     0.0|   1.0|  male|54.0|51.8625|\n",
      "|     0.0|   3.0|  male| 2.0| 21.075|\n",
      "|     1.0|   3.0|female|27.0|11.1333|\n",
      "|     1.0|   2.0|female|14.0|30.0708|\n",
      "|     1.0|   3.0|female| 4.0|   16.7|\n",
      "|     1.0|   1.0|female|58.0|  26.55|\n",
      "|     0.0|   3.0|  male|20.0|   8.05|\n",
      "|     0.0|   3.0|  male|39.0| 31.275|\n",
      "|     0.0|   3.0|female|14.0| 7.8542|\n",
      "|     1.0|   2.0|female|55.0|   16.0|\n",
      "|     0.0|   3.0|  male| 2.0| 29.125|\n",
      "|     1.0|   2.0|  male|23.0|   13.0|\n",
      "|     0.0|   3.0|female|31.0|   18.0|\n",
      "|     1.0|   3.0|female|22.0|  7.225|\n",
      "+--------+------+------+----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Â Typecast columns. Survived is the target variable: \n",
    "dataset = df.select(col('Survived').cast('float'),\n",
    "                         col('Pclass').cast('float'),\n",
    "                         col('Sex'),\n",
    "                         col('Age').cast('float'),\n",
    "                         col('Fare').cast('float')\n",
    "                        )\n",
    "\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+-------+------+\n",
      "|Survived|Pclass| Age|   Fare|Gender|\n",
      "+--------+------+----+-------+------+\n",
      "|     0.0|   3.0|22.0|   7.25|   0.0|\n",
      "|     1.0|   1.0|38.0|71.2833|   1.0|\n",
      "|     1.0|   3.0|26.0|  7.925|   1.0|\n",
      "|     1.0|   1.0|35.0|   53.1|   1.0|\n",
      "|     0.0|   3.0|35.0|   8.05|   0.0|\n",
      "|     0.0|   3.0|27.0| 8.4583|   0.0|\n",
      "|     0.0|   1.0|54.0|51.8625|   0.0|\n",
      "|     0.0|   3.0| 2.0| 21.075|   0.0|\n",
      "|     1.0|   3.0|27.0|11.1333|   1.0|\n",
      "|     1.0|   2.0|14.0|30.0708|   1.0|\n",
      "|     1.0|   3.0| 4.0|   16.7|   1.0|\n",
      "|     1.0|   1.0|58.0|  26.55|   1.0|\n",
      "|     0.0|   3.0|20.0|   8.05|   0.0|\n",
      "|     0.0|   3.0|39.0| 31.275|   0.0|\n",
      "|     0.0|   3.0|14.0| 7.8542|   1.0|\n",
      "|     1.0|   2.0|55.0|   16.0|   1.0|\n",
      "|     0.0|   3.0| 2.0| 29.125|   0.0|\n",
      "|     1.0|   2.0|23.0|   13.0|   0.0|\n",
      "|     0.0|   3.0|31.0|   18.0|   1.0|\n",
      "|     1.0|   3.0|22.0|  7.225|   1.0|\n",
      "+--------+------+----+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Index categorical columns with StringIndexer\n",
    "\n",
    "dataset = StringIndexer(inputCol='Sex', \n",
    "                        outputCol='Gender', \n",
    "                        handleInvalid='keep').fit(dataset).transform(dataset)\n",
    "\n",
    "# drop columns \n",
    "dataset = dataset.drop('Sex')\n",
    "dataset = dataset.drop('Embarked')\n",
    "\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a simple Pipeline Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input features: \n",
    "required_features = ['Pclass',\n",
    "                    'Age',\n",
    "                    'Fare',\n",
    "                    'Gender']\n",
    "\n",
    "# 80/20 data split \n",
    "(training_data, test_data) = dataset.randomSplit([0.8,0.2])\n",
    "\n",
    "assembler = VectorAssembler(inputCols=required_features, outputCol='features')\n",
    "rf = RandomForestClassifier(labelCol='Survived', \n",
    "                            featuresCol='features',\n",
    "                            maxDepth=5)\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = pipeline.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+-------+------+--------------------+--------------------+--------------------+----------+\n",
      "|Survived|Pclass| Age|   Fare|Gender|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+------+----+-------+------+--------------------+--------------------+--------------------+----------+\n",
      "|     0.0|   3.0|22.0|   7.25|   0.0| [3.0,22.0,7.25,0.0]|[17.8566101677140...|[0.89283050838570...|       0.0|\n",
      "|     1.0|   1.0|38.0|71.2833|   1.0|[1.0,38.0,71.2833...|[0.99662422569703...|[0.04983121128485...|       1.0|\n",
      "|     1.0|   3.0|26.0|  7.925|   1.0|[3.0,26.0,7.92500...|[9.43397278245809...|[0.47169863912290...|       1.0|\n",
      "|     1.0|   1.0|35.0|   53.1|   1.0|[1.0,35.0,53.0999...|[1.13394024301305...|[0.05669701215065...|       1.0|\n",
      "|     0.0|   3.0|35.0|   8.05|   0.0|[3.0,35.0,8.05000...|[17.8264474683963...|[0.89132237341981...|       0.0|\n",
      "|     0.0|   3.0|27.0| 8.4583|   0.0|[3.0,27.0,8.45829...|[17.7496419822158...|[0.88748209911079...|       0.0|\n",
      "|     0.0|   1.0|54.0|51.8625|   0.0|[1.0,54.0,51.8624...|[12.9374178029978...|[0.64687089014989...|       0.0|\n",
      "|     0.0|   3.0| 2.0| 21.075|   0.0|[3.0,2.0,21.07500...|[12.8015185207669...|[0.64007592603834...|       0.0|\n",
      "|     1.0|   3.0|27.0|11.1333|   1.0|[3.0,27.0,11.1332...|[6.37562067615212...|[0.31878103380760...|       1.0|\n",
      "|     1.0|   2.0|14.0|30.0708|   1.0|[2.0,14.0,30.0708...|[1.48533128158783...|[0.07426656407939...|       1.0|\n",
      "+--------+------+----+-------+------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = pipeline_model.transform(dataset)\n",
    "results.limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Model Object to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_dir = \"./model-artifact\"\n",
    "\n",
    "if not os.path.exists(artifact_dir):\n",
    "    os.makedirs(artifact_dir)\n",
    "pipeline_model.write().overwrite().save(f\"{artifact_dir}/my-model\")\n",
    "#pipeline_model.write().overwrite().save(\"oci://<my-bucket>@<my-namespace/my-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing a model artifact with ADS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='loop1'), FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:ADS:We give you the option to specify a different inference conda environment for model deployment purposes. By default it is assumed to be the same as the conda environment used to train the model. If you wish to specify a different environment for inference purposes, please assign the path of a published or data science conda environment to the optional parameter `inference_conda_env`. \n"
     ]
    }
   ],
   "source": [
    "ads_artifact = prepare_generic_model(artifact_dir, \n",
    "                                     force_overwrite=True, \n",
    "                                     function_artifacts=False, \n",
    "                                     data_science_env=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overwriting score.py to support spark models \n",
    "\n",
    "Added a bunch of print statements for logging purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./model-artifact/score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {artifact_dir}/score.py\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "sc = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('Titanic2') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\"\"\"\n",
    "   Inference script. This script is used for prediction by scoring server when schema is known.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"\n",
    "    Loads model from the serialized format\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model:  a model instance on which predict API can be invoked\n",
    "    \"\"\"\n",
    "    print('loading model')\n",
    "    # use this path for model deployment: \n",
    "    pm2 = PipelineModel.load('/home/datascience/model-server/app/deployed_model/my-model')\n",
    "    # use this path for testing locally: \n",
    "    #pm2 = PipelineModel.load('/home/datascience/sparkml/model-artifact/my-model/')\n",
    " \n",
    "    print('done reading model')\n",
    "\n",
    "    print(pm2.__class__)\n",
    "    return pm2\n",
    "\n",
    "\n",
    "def predict(data, model=load_model()) -> dict:\n",
    "    \"\"\"\n",
    "    Returns prediction given the model and data to predict\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: Model instance returned by load_model API\n",
    "    data: Data format as expected by the predict API of the core estimator. For eg. in case of sckit models it could be numpy array/List of list/Panda DataFrame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predictions: Output from scoring server\n",
    "        Format: { 'prediction': output from `model.predict` method }\n",
    "\n",
    "    \"\"\"\n",
    "    print('before reading data')\n",
    "    tmp = sc.read.json(sc.sparkContext.parallelize([data]))\n",
    "    print(tmp.show())\n",
    "    print(tmp.printSchema())\n",
    "    print('after reading data')\n",
    "    tmp = tmp.select(col('Pclass').cast('float'),\n",
    "                      col('Gender').cast('double'),\n",
    "                      col('Age').cast('float'),\n",
    "                      col('Fare').cast('float')\n",
    "                     )\n",
    "    print('after selecting subset of columns and typecasting')   \n",
    "    results = model.transform(tmp)\n",
    "    print('after prediction') \n",
    "    print(results.show())\n",
    "    res = results.toPandas()['prediction']\n",
    "    print(\"prediction = \", res)\n",
    "    return { 'prediction': str(list(res)) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model locally "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.insert(0, artifact_dir)\n",
    "\n",
    "from score import load_model, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model\n",
      "done reading model\n",
      "<class 'pyspark.ml.pipeline.PipelineModel'>\n"
     ]
    }
   ],
   "source": [
    "_ = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample payload as a json string\n",
    "test = test_data.toPandas()\n",
    "testjson = test[:1].to_json(orient='records',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"Survived\":0.0,\"Pclass\":1.0,\"Age\":18.0,\"Fare\":108.9000015259,\"Gender\":0.0}'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before reading data\n",
      "+---+--------------+------+------+--------+\n",
      "|Age|          Fare|Gender|Pclass|Survived|\n",
      "+---+--------------+------+------+--------+\n",
      "|2.0|151.5500030518|   1.0|   1.0|     0.0|\n",
      "+---+--------------+------+------+--------+\n",
      "\n",
      "None\n",
      "root\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Gender: double (nullable = true)\n",
      " |-- Pclass: double (nullable = true)\n",
      " |-- Survived: double (nullable = true)\n",
      "\n",
      "None\n",
      "after reading data\n",
      "after selecting subset of columns and typecasting\n",
      "after prediction\n",
      "+------+------+---+------+--------------------+--------------------+--------------------+----------+\n",
      "|Pclass|Gender|Age|  Fare|            features|       rawPrediction|         probability|prediction|\n",
      "+------+------+---+------+--------------------+--------------------+--------------------+----------+\n",
      "|   1.0|   1.0|2.0|151.55|[1.0,2.0,151.5500...|[0.31171453926536...|[0.01558572696326...|       1.0|\n",
      "+------+------+---+------+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "None\n",
      "prediction =  0    1.0\n",
      "Name: prediction, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "res = predict(testjson, _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the artifact to the Catalog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:ADS:{\n",
      "  \"git_branch\": \"None\",\n",
      "  \"git_commit\": \"None\",\n",
      "  \"repository_url\": \"None\",\n",
      "  \"script_dir\": \"/home/datascience/sparkml/model-artifact\",\n",
      "  \"training_script\": \"None\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='loop1'), FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_afb41692_1ced_11ec_ba27_0242ac130002row0_col0,#T_afb41692_1ced_11ec_ba27_0242ac130002row1_col0,#T_afb41692_1ced_11ec_ba27_0242ac130002row2_col0,#T_afb41692_1ced_11ec_ba27_0242ac130002row3_col0,#T_afb41692_1ced_11ec_ba27_0242ac130002row4_col0,#T_afb41692_1ced_11ec_ba27_0242ac130002row5_col0,#T_afb41692_1ced_11ec_ba27_0242ac130002row6_col0,#T_afb41692_1ced_11ec_ba27_0242ac130002row7_col0,#T_afb41692_1ced_11ec_ba27_0242ac130002row8_col0,#T_afb41692_1ced_11ec_ba27_0242ac130002row9_col0,#T_afb41692_1ced_11ec_ba27_0242ac130002row10_col0,#T_afb41692_1ced_11ec_ba27_0242ac130002row11_col0,#T_afb41692_1ced_11ec_ba27_0242ac130002row12_col0,#T_afb41692_1ced_11ec_ba27_0242ac130002row13_col0,#T_afb41692_1ced_11ec_ba27_0242ac130002row14_col0,#T_afb41692_1ced_11ec_ba27_0242ac130002row15_col0{\n",
       "            margin-left:  0px;\n",
       "        }</style><table id=\"T_afb41692_1ced_11ec_ba27_0242ac130002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" ></th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_afb41692_1ced_11ec_ba27_0242ac130002level0_row0\" class=\"row_heading level0 row0\" >id</th>\n",
       "                        <td id=\"T_afb41692_1ced_11ec_ba27_0242ac130002row0_col0\" class=\"data row0 col0\" >ocid1.datasciencemodel.oc1.iad.amaaaaaanif7xwiachppmyp5sgixa56ffgtweh3kki6mt3trc2uavab5ssjq</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_afb41692_1ced_11ec_ba27_0242ac130002level0_row1\" class=\"row_heading level0 row1\" >compartment_id</th>\n",
       "                        <td id=\"T_afb41692_1ced_11ec_ba27_0242ac130002row1_col0\" class=\"data row1 col0\" >ocid1.compartment.oc1..aaaaaaaalcio324mqxi6egudwmc2wzix3yclcysmmji4cggvnj4b5timvw2q</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_afb41692_1ced_11ec_ba27_0242ac130002level0_row2\" class=\"row_heading level0 row2\" >project_id</th>\n",
       "                        <td id=\"T_afb41692_1ced_11ec_ba27_0242ac130002row2_col0\" class=\"data row2 col0\" >ocid1.datascienceproject.oc1.iad.amaaaaaanif7xwiaot7v42xns7rha7cfvno76gzn4n2yhknw5c4i3jo5wpfq</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_afb41692_1ced_11ec_ba27_0242ac130002level0_row3\" class=\"row_heading level0 row3\" >display_name</th>\n",
       "                        <td id=\"T_afb41692_1ced_11ec_ba27_0242ac130002row3_col0\" class=\"data row3 col0\" >my-spark-model</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_afb41692_1ced_11ec_ba27_0242ac130002level0_row4\" class=\"row_heading level0 row4\" >description</th>\n",
       "                        <td id=\"T_afb41692_1ced_11ec_ba27_0242ac130002row4_col0\" class=\"data row4 col0\" >pipeline object</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_afb41692_1ced_11ec_ba27_0242ac130002level0_row5\" class=\"row_heading level0 row5\" >lifecycle_state</th>\n",
       "                        <td id=\"T_afb41692_1ced_11ec_ba27_0242ac130002row5_col0\" class=\"data row5 col0\" >ACTIVE</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_afb41692_1ced_11ec_ba27_0242ac130002level0_row6\" class=\"row_heading level0 row6\" >time_created</th>\n",
       "                        <td id=\"T_afb41692_1ced_11ec_ba27_0242ac130002row6_col0\" class=\"data row6 col0\" >2021-09-24 04:12:51.346000+00:00</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_afb41692_1ced_11ec_ba27_0242ac130002level0_row7\" class=\"row_heading level0 row7\" >created_by</th>\n",
       "                        <td id=\"T_afb41692_1ced_11ec_ba27_0242ac130002row7_col0\" class=\"data row7 col0\" >ocid1.user.oc1..aaaaaaaabfrlcbiyvjmjvgh3ns6trdyoewxytqywwta3yqmy3ah3fa3uw76q</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_afb41692_1ced_11ec_ba27_0242ac130002level0_row8\" class=\"row_heading level0 row8\" >freeform_tags</th>\n",
       "                        <td id=\"T_afb41692_1ced_11ec_ba27_0242ac130002row8_col0\" class=\"data row8 col0\" >{}</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_afb41692_1ced_11ec_ba27_0242ac130002level0_row9\" class=\"row_heading level0 row9\" >defined_tags</th>\n",
       "                        <td id=\"T_afb41692_1ced_11ec_ba27_0242ac130002row9_col0\" class=\"data row9 col0\" >{'Oracle-Tags': {'CreatedBy': 'jr_local', 'CreatedOn': '2021-09-24T04:12:51.204Z'}}</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_afb41692_1ced_11ec_ba27_0242ac130002level0_row10\" class=\"row_heading level0 row10\" >user_name</th>\n",
       "                        <td id=\"T_afb41692_1ced_11ec_ba27_0242ac130002row10_col0\" class=\"data row10 col0\" >jr_local</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_afb41692_1ced_11ec_ba27_0242ac130002level0_row11\" class=\"row_heading level0 row11\" >repository_url</th>\n",
       "                        <td id=\"T_afb41692_1ced_11ec_ba27_0242ac130002row11_col0\" class=\"data row11 col0\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_afb41692_1ced_11ec_ba27_0242ac130002level0_row12\" class=\"row_heading level0 row12\" >git_branch</th>\n",
       "                        <td id=\"T_afb41692_1ced_11ec_ba27_0242ac130002row12_col0\" class=\"data row12 col0\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_afb41692_1ced_11ec_ba27_0242ac130002level0_row13\" class=\"row_heading level0 row13\" >git_commit</th>\n",
       "                        <td id=\"T_afb41692_1ced_11ec_ba27_0242ac130002row13_col0\" class=\"data row13 col0\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_afb41692_1ced_11ec_ba27_0242ac130002level0_row14\" class=\"row_heading level0 row14\" >script_dir</th>\n",
       "                        <td id=\"T_afb41692_1ced_11ec_ba27_0242ac130002row14_col0\" class=\"data row14 col0\" >/home/datascience/sparkml/model-artifact</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_afb41692_1ced_11ec_ba27_0242ac130002level0_row15\" class=\"row_heading level0 row15\" >training_script</th>\n",
       "                        <td id=\"T_afb41692_1ced_11ec_ba27_0242ac130002row15_col0\" class=\"data row15 col0\" >None</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "{\n",
       "  \"compartment_id\": \"ocid1.compartment.oc1..aaaaaaaalcio324mqxi6egudwmc2wzix3yclcysmmji4cggvnj4b5timvw2q\",\n",
       "  \"created_by\": \"ocid1.user.oc1..aaaaaaaabfrlcbiyvjmjvgh3ns6trdyoewxytqywwta3yqmy3ah3fa3uw76q\",\n",
       "  \"defined_tags\": {\n",
       "    \"Oracle-Tags\": {\n",
       "      \"CreatedBy\": \"jr_local\",\n",
       "      \"CreatedOn\": \"2021-09-24T04:12:51.204Z\"\n",
       "    }\n",
       "  },\n",
       "  \"description\": \"pipeline object\",\n",
       "  \"display_name\": \"my-spark-model\",\n",
       "  \"freeform_tags\": {},\n",
       "  \"id\": \"ocid1.datasciencemodel.oc1.iad.amaaaaaanif7xwiachppmyp5sgixa56ffgtweh3kki6mt3trc2uavab5ssjq\",\n",
       "  \"lifecycle_state\": \"ACTIVE\",\n",
       "  \"project_id\": \"ocid1.datascienceproject.oc1.iad.amaaaaaanif7xwiaot7v42xns7rha7cfvno76gzn4n2yhknw5c4i3jo5wpfq\",\n",
       "  \"time_created\": \"2021-09-24T04:12:51.346000+00:00\",\n",
       "  \"user_name\": \"jr_local\"\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ads_artifact.save(display_name=\"my-spark-model\", \n",
    "                  description=\"pipeline object\",\n",
    "                  project_id=os.environ['PROJECT_OCID'],\n",
    "                  compartment_id=os.environ['NB_SESSION_COMPARTMENT_OCID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the model through the console "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoke the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': '[0.0]'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import oci\n",
    "from oci.signer import Signer\n",
    "\n",
    "# User principal auth: \n",
    "#config = oci.config.from_file(\"~/.oci/config\") # replace with the location of your oci config file\n",
    "#auth = Signer(\n",
    "#  tenancy=config['tenancy'],\n",
    "#  user=config['user'],\n",
    "#  fingerprint=config['fingerprint'],\n",
    "#  private_key_file_location=config['key_file'],\n",
    "#  pass_phrase=config['pass_phrase'])\n",
    "\n",
    "# Resource principal auth:\n",
    "auth = oci.auth.signers.get_resource_principals_signer()\n",
    "\n",
    "# replace with your own endpoint: \n",
    "endpoint = 'https://modeldeployment.us-ashburn-1.oci.customer-oci.com/ocid1.datasciencemodeldeployment.oc1.iad.amaaaaaanif7xwiaxyz2jly5hd6lofjpa7e7sinzs6gzcb6mt5bnb5sax43a/predict'\n",
    "\n",
    "requests.post(endpoint, json=testjson, auth=auth).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 81.6 ms, sys: 15.2 ms, total: 96.8 ms\n",
      "Wall time: 3.29 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prediction': '[0.0]'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "requests.post(endpoint, json=testjson, auth=auth).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspv10]",
   "language": "python",
   "name": "conda-env-pyspv10-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
